{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kR0SKBDbaOqR"
   },
   "source": [
    "# Neural Graphics Ex1: Training Your Own Diffusion Model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryMrLOORbWLz"
   },
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lX3XpcGSXBIs",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5dc23e69-96b7-4722-d842-dbc108ce5522",
    "ExecuteTime": {
     "end_time": "2025-06-11T16:19:36.634401Z",
     "start_time": "2025-06-11T16:19:36.629290Z"
    }
   },
   "source": [
    "# We recommend using these utils.\n",
    "# https://google.github.io/mediapy/mediapy.html\n",
    "# https://einops.rocks/\n",
    "# !pip install mediapy einops --quiet"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VdFQ6c9-Pm4Y",
    "ExecuteTime": {
     "end_time": "2025-06-11T16:19:36.645791Z",
     "start_time": "2025-06-11T16:19:36.639213Z"
    }
   },
   "source": [
    "# Import essential modules. Feel free to add whatever you need.\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sympy.polys.polyoptions import Expand\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cu118\n",
      "True\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Seed your work\n",
    "To be able to reproduce your code, please use a random seed from this point onward."
   ],
   "metadata": {
    "id": "pDJ3QzHdRV52"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def seed_everything(seed):\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "YOUR_SEED = 180  # modify if you want\n",
    "seed_everything(YOUR_SEED)"
   ],
   "metadata": {
    "id": "aDVpoyjaRcoC",
    "ExecuteTime": {
     "end_time": "2025-06-11T16:19:36.665879Z",
     "start_time": "2025-06-11T16:19:36.659264Z"
    }
   },
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dokvxybn_DwK"
   },
   "source": [
    "## 1. Basic Ops and UNet blocks\n",
    "**Notations:**  \n",
    " * `Conv2D(kernel_size, stride, padding)` is `nn.Conv2d()`  \n",
    " * `BN` is `nn.BatchNorm2d()`  \n",
    " * `GELU` is `nn.GELU()`  \n",
    " * `ConvTranspose2D(kernel_size, stride, padding)` is `nn.ConvTranspose2d()`  \n",
    " * `AvgPool(kernel_size)` is `nn.AvgPool2d()`  \n",
    " * `Linear` is `nn.Linear()`  \n",
    " * `N`, `C`, `W` and `H` are batch size, channels num, weight and height respectively\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Basic Ops"
   ],
   "metadata": {
    "id": "k1iFNqV8HjzM"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fhpEzgwCJqbW",
    "ExecuteTime": {
     "end_time": "2025-06-11T16:19:36.714271Z",
     "start_time": "2025-06-11T16:19:36.699445Z"
    }
   },
   "source": [
    "class Conv(nn.Module):\n",
    "    \"\"\"\n",
    "    A convolutional layer that doesn’t change the image\n",
    "    resolution, only the channel dimension\n",
    "    Applies nn.Conv2d(3, 1, 1) followed by BN and GELU.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        \"\"\"\n",
    "        Initializes the Conv layer\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels\n",
    "            out_channels (int): The number of output channels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv2d = nn.Conv2d(kernel_size=3,\n",
    "                                padding=1,\n",
    "                                stride=1,\n",
    "                                in_channels=in_channels,\n",
    "                                out_channels=out_channels)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (N, in_channels, H, W) input tensor.\n",
    "\n",
    "        Returns:\n",
    "            (N, out_channels, H, W) output tensor.\n",
    "        \"\"\"\n",
    "        x = self.conv2d(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.gelu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DownConv(nn.Module):\n",
    "    \"\"\"\n",
    "        A convolutional layer down-samples the tensor by 2.\n",
    "        The layer consists of Conv2D(3, 2, 1) followed by BN and GELU.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        \"\"\"\n",
    "        Initializes the DownConv layer\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels\n",
    "            out_channels (int): The number of output channels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv2d = nn.Conv2d(kernel_size=3,\n",
    "                                stride=2,\n",
    "                                padding=1,\n",
    "                                in_channels=in_channels,\n",
    "                                out_channels=out_channels)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (N, in_channels, H, W) input tensor.\n",
    "\n",
    "        Returns:\n",
    "            (N, out_channels, H/2, W/2) output tensor.\n",
    "        \"\"\"\n",
    "        x = self.conv2d(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.gelu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UpConv(nn.Module):\n",
    "    \"\"\"\n",
    "    A convolutional layer that upsamples the tensor by 2.\n",
    "    The layer consists of ConvTranspose2d(4, 2, 1) followed by\n",
    "    BN and GELU.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        \"\"\"\n",
    "        Initializes the UpConv layer\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels\n",
    "            out_channels (int): The number of output channels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.convTranspose2d = nn.ConvTranspose2d(kernel_size=4,\n",
    "                                                  stride=2,\n",
    "                                                  padding=1,\n",
    "                                                  in_channels=in_channels,\n",
    "                                                  out_channels=out_channels)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (N, in_channels, H, W) input tensor.\n",
    "\n",
    "        Returns:\n",
    "            (N, out_channels, H*2, W*2) output tensor.\n",
    "        \"\"\"\n",
    "        x = self.convTranspose2d(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.gelu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    \"\"\"\n",
    "    Average pooling layer that flattens a 7x7 tensor into a 1x1 tensor.\n",
    "    The layer consists of AvgPool followed by GELU.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.avgpool = nn.AvgPool2d(7)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (N, C, 7, 7) input tensor.\n",
    "\n",
    "        Returns:\n",
    "            (N, C, 1, 1) output tensor.\n",
    "        \"\"\"\n",
    "        x = self.avgpool(x)\n",
    "        x = self.gelu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Unflatten(nn.Module):\n",
    "    \"\"\"\n",
    "      Convolutional layer that expends/up-samples a 1x1 tensor into a\n",
    "      7x7 tensor. The layer consists of ConvTranspose2D(7, 7, 0)\n",
    "      followed by BN and GELU.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int):\n",
    "        \"\"\"\n",
    "        Initializes Unflatten layer\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.convtranspose2d = nn.ConvTranspose2d(kernel_size=7,\n",
    "                                                  stride=7,\n",
    "                                                  padding=0,\n",
    "                                                  in_channels=in_channels,\n",
    "                                                  out_channels=in_channels)\n",
    "        self.bn = nn.BatchNorm2d(in_channels)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (N, in_channels, 1, 1) input tensor.\n",
    "\n",
    "        Returns:\n",
    "            (N, in_channels, 7, 7) output tensor.\n",
    "        \"\"\"\n",
    "        x = self.convtranspose2d(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.gelu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FC(nn.Module):\n",
    "    \"\"\"\n",
    "    Fully connected layer, consisting of nn.linear followed by GELU.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        \"\"\"\n",
    "        Initializes the FC layer\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels\n",
    "            out_channels (int): The number of output channels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_channels, out_channels)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (N, in_channels) input tensor.\n",
    "\n",
    "        Returns:\n",
    "            (N, out_channels) output tensor.\n",
    "        \"\"\"\n",
    "        x = self.fc(x)\n",
    "        x = self.gelu(x)\n",
    "        return x\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "source": [
    "### UNet Blocks"
   ],
   "metadata": {
    "id": "GCawfhu0HqcH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Two consecutive Conv operations.\n",
    "    Note that it has the same input and output shape as Conv.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        \"\"\"\n",
    "        Initializes ConvBlock\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels\n",
    "            out_channels (int): The number of output channels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv(in_channels, out_channels)\n",
    "        self.conv2 = Conv(out_channels, out_channels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (N, in_channels, H, W) input tensor.\n",
    "\n",
    "        Returns:\n",
    "            (N, out_channels, H, W) output tensor.\n",
    "        \"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    DownConv followed by ConvBlock. Note that it has the same input and output\n",
    "    shape as DownConv.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        \"\"\"\n",
    "        Initializes DownBlock\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels\n",
    "            out_channels (int): The number of output channels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv1 = DownConv(in_channels, out_channels)\n",
    "        self.conv2 = ConvBlock(out_channels, out_channels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (N, in_channels, H, W) input tensor.\n",
    "\n",
    "        Returns:\n",
    "            (N, out_channels, H/2, W/2) output tensor.\n",
    "        \"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    UpConv followed by ConvBlock.\n",
    "    Note that it has the same input and output shape as UpConv\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        \"\"\"\n",
    "        Initializes UpBlock\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels\n",
    "            out_channels (int): The number of output channels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv1 = UpConv(in_channels, out_channels)\n",
    "        self.conv2 = ConvBlock(out_channels, out_channels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (N, in_channels, H, W) input tensor.\n",
    "\n",
    "        Returns:\n",
    "            (N, out_channels, H*2, W*2) output tensor.\n",
    "        \"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FCBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Fully-connected Block, consisting of FC layer followed by Linear layer. Note\n",
    "    that it has the same input and output shape as FC.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        \"\"\"\n",
    "        Initializes FCBlock\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels\n",
    "            out_channels (int): The number of output channels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.fc = FC(in_channels, out_channels)\n",
    "        self.linear = nn.Linear(out_channels, out_channels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (N, in_channels) input tensor.\n",
    "\n",
    "        Returns:\n",
    "            (N, out_channels) output tensor.\n",
    "        \"\"\"\n",
    "        x = self.fc(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ],
   "metadata": {
    "id": "3nusVuFlHt67",
    "ExecuteTime": {
     "end_time": "2025-06-11T16:19:36.728148Z",
     "start_time": "2025-06-11T16:19:36.719220Z"
    }
   },
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMI3IMkjayxQ"
   },
   "source": [
    "## 2. Unconditional Diffusion Framework\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 UNet architecture"
   ],
   "metadata": {
    "id": "t9JhNQN5Ad3V"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fkchbyYkzAvV",
    "ExecuteTime": {
     "end_time": "2025-06-11T16:19:36.790807Z",
     "start_time": "2025-06-11T16:19:36.755445Z"
    }
   },
   "source": [
    "class DenoisingUNet(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels: int,  # 1\n",
    "            num_hiddens: int  # D\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # t-tensor\n",
    "        self.t_fc4 = FCBlock(in_channels, 2 * num_hiddens)\n",
    "        self.t_fc2 = FCBlock(in_channels, num_hiddens)\n",
    "\n",
    "        # In\n",
    "        self.conv_block_in = ConvBlock(in_channels, num_hiddens)  # (N, D, 28, 28)\n",
    "        # Down\n",
    "        self.down_block1 = DownBlock(num_hiddens, num_hiddens)  # (N, D, 14, 14)\n",
    "        self.down_block2 = DownBlock(num_hiddens, 2 * num_hiddens)  # (N, 2 * D, 7, 7)\n",
    "        self.flatten = Flatten()  # (N, 2 * D, 1, 1)\n",
    "        # Up / with skip connections and t-tensor addition\n",
    "        self.expend = Unflatten(2 * num_hiddens)  # (N, [2 * D + 2 * D], 7, 7)\n",
    "        self.up_block2 = UpBlock(4 * num_hiddens, num_hiddens)  # (N, [D + D], 14, 14)\n",
    "        self.up_block1 = UpBlock(2 * num_hiddens, num_hiddens)  # (N, [D + D], 28, 28)\n",
    "        # Out\n",
    "        self.conv_block_out = ConvBlock(2 * num_hiddens, num_hiddens)  # (N, D, 28, 28)\n",
    "        self.conv2d = Conv(num_hiddens, 1)  #  (N, 1, 28, 28)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            x: torch.Tensor,\n",
    "            t: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (N, C, H, W) input tensor.\n",
    "            t: (N, 1) normalized time tensor.\n",
    "\n",
    "        Returns:\n",
    "            (N, C, H, W) output tensor.\n",
    "        \"\"\"\n",
    "        assert x.shape[-2:] == (28, 28), \"Expect input shape to be (28, 28).\"\n",
    "        # t-tensor\n",
    "        t_layer4 = self.t_fc4(t).unsqueeze(-1).unsqueeze(-1)\n",
    "        t_layer2 = self.t_fc2(t).unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        # Down\n",
    "        d_layer1 = self.conv_block_in(x)\n",
    "        d_layer2 = self.down_block1(d_layer1)\n",
    "        d_layer3 = self.down_block2(d_layer2)\n",
    "\n",
    "        # Flatten/Extract\n",
    "        flat = self.flatten(d_layer3)\n",
    "        expend = self.expend(flat)\n",
    "\n",
    "        expend += t_layer4  # t-tensor addition layer 4\n",
    "\n",
    "        # Up\n",
    "        u_layer2 = self.up_block2(torch.cat((d_layer3, expend), dim=1))  # skip connection layer 3\n",
    "        u_layer2 += t_layer2  # t-tensor addition layer 2\n",
    "\n",
    "        u_layer1 = self.up_block1(torch.cat((d_layer2, u_layer2), dim=1))  # skip connection layer 2\n",
    "        output = self.conv_block_out(torch.cat((d_layer1, u_layer1), dim=1))  # skip connection layer 1\n",
    "        output = self.conv2d(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "data = torch.randn(10, 1, 28, 28)  # batch, channel, height, width\n",
    "t = torch.randn(10, 1)\n",
    "model = DenoisingUNet(data.shape[1], 10)\n",
    "model.forward(x=data, t=t)\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.5900,  0.5139,  0.5533,  ...,  0.1197,  0.5582,  0.2314],\n",
       "          [ 0.8292,  0.6498,  0.7629,  ...,  0.2697,  0.2929,  0.5280],\n",
       "          [ 0.4137,  0.1206,  0.3123,  ...,  0.7934,  0.1878,  0.1778],\n",
       "          ...,\n",
       "          [ 0.6513, -0.0946, -0.1580,  ..., -0.1355, -0.0608,  0.1265],\n",
       "          [ 0.1745,  0.3268, -0.1534,  ...,  0.0405,  0.1543,  0.3352],\n",
       "          [ 0.4307,  0.3598, -0.0025,  ..., -0.1144,  0.1708,  0.3015]]],\n",
       "\n",
       "\n",
       "        [[[ 0.5589,  0.7514,  0.4481,  ...,  0.4048,  0.4661,  0.2583],\n",
       "          [ 0.3091,  1.1313,  0.0553,  ...,  0.8931,  0.2321,  0.1722],\n",
       "          [-0.0697,  0.1088,  0.2607,  ...,  0.6474,  0.2348,  0.0376],\n",
       "          ...,\n",
       "          [-0.1251,  0.0180,  0.5869,  ..., -0.1507,  1.1511,  0.4065],\n",
       "          [ 0.5442,  0.4756,  0.4075,  ...,  1.2067,  0.4984,  0.0849],\n",
       "          [ 1.0770, -0.1525,  0.0429,  ...,  0.7998,  0.5454,  0.5100]]],\n",
       "\n",
       "\n",
       "        [[[ 0.4541,  0.2706, -0.1018,  ..., -0.1352,  0.6679, -0.1202],\n",
       "          [ 0.4426,  0.5841, -0.1175,  ...,  0.2883,  0.0261,  0.0614],\n",
       "          [ 0.2591,  0.1108, -0.1012,  ...,  0.8061, -0.0186,  0.1377],\n",
       "          ...,\n",
       "          [ 0.3159,  0.3940,  1.2540,  ...,  0.4296, -0.1369, -0.1700],\n",
       "          [ 0.9039,  0.4875,  1.0461,  ...,  0.3345, -0.1542,  0.1675],\n",
       "          [ 0.4216,  0.6511,  0.9075,  ...,  0.2677, -0.0316,  0.4365]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 0.2428,  0.1940,  0.3897,  ...,  0.7032,  0.5762,  0.2488],\n",
       "          [ 0.7059,  0.4590,  0.0123,  ...,  0.4736,  0.8157,  0.4749],\n",
       "          [ 0.4631, -0.1658, -0.1098,  ..., -0.0146,  1.0484,  0.5187],\n",
       "          ...,\n",
       "          [ 0.1647,  0.4835,  0.9731,  ..., -0.0935,  0.1801, -0.1672],\n",
       "          [ 0.1230,  0.4905,  0.4799,  ...,  0.3387, -0.1022, -0.1301],\n",
       "          [ 0.6473,  0.4092,  0.7498,  ...,  0.3736, -0.0619, -0.0078]]],\n",
       "\n",
       "\n",
       "        [[[ 0.6850,  0.2067,  0.0750,  ...,  1.2346,  0.3669,  0.7664],\n",
       "          [ 0.6740,  0.2770,  0.3171,  ...,  0.9929,  0.9040,  0.9406],\n",
       "          [ 0.1949,  0.2695,  0.0107,  ...,  1.1201,  0.4840,  0.2661],\n",
       "          ...,\n",
       "          [ 0.9100, -0.1415,  0.1271,  ..., -0.0245,  0.7851, -0.0165],\n",
       "          [ 0.8090, -0.0174, -0.1679,  ...,  0.1895,  0.5002,  0.1377],\n",
       "          [ 0.3777,  0.5762,  0.0833,  ...,  0.4857,  0.5578,  0.2107]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2358,  1.0420,  0.6320,  ...,  1.0802,  0.0725,  0.3750],\n",
       "          [ 0.8706,  1.0153,  0.0654,  ...,  1.1729,  0.0679, -0.0414],\n",
       "          [ 0.7396, -0.1226, -0.0974,  ...,  1.3657, -0.1700,  0.2268],\n",
       "          ...,\n",
       "          [ 0.9603,  0.4329,  0.3411,  ..., -0.1663,  0.4402, -0.0202],\n",
       "          [ 0.4045,  0.6143, -0.1144,  ...,  0.9458,  0.5059,  0.1546],\n",
       "          [ 0.2836,  1.0010, -0.1474,  ...,  0.5589,  0.1516,  0.4645]]]],\n",
       "       grad_fn=<GeluBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nyxOM-RbZnC"
   },
   "source": "### 2.2 DDPM Forward and Inverse Process\n"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yIvMw63T6JkE",
    "ExecuteTime": {
     "end_time": "2025-06-11T16:19:36.801621Z",
     "start_time": "2025-06-11T16:19:36.796745Z"
    }
   },
   "source": [
    "def ddpm_schedule(beta1: float, beta2: float, num_ts: int, device: str = 'cuda') -> dict:\n",
    "    \"\"\"Constants for DDPM training and sampling.\n",
    "\n",
    "    Arguments:\n",
    "        beta1: float, starting beta value.\n",
    "        beta2: float, ending beta value.\n",
    "        num_ts: int, number of timesteps.\n",
    "\n",
    "    Returns:\n",
    "        dict with keys:\n",
    "            betas: linear schedule of betas from beta1 to beta2.\n",
    "            alphas: 1 - betas.\n",
    "            alpha_bars: cumulative product of alphas.\n",
    "    \"\"\"\n",
    "    assert beta1 < beta2 < 1.0, \"Expect beta1 < beta2 < 1.0.\"\n",
    "\n",
    "    beta_seq = torch.linspace(beta1, beta2, num_ts, device=device)\n",
    "    alpha_seq = 1.0 - beta_seq\n",
    "    alpha_bar_seq = torch.cumprod(alpha_seq, dim=0)\n",
    "\n",
    "    return {\n",
    "        \"beta_seq\": beta_seq,\n",
    "        \"alpha_seq\": alpha_seq,\n",
    "        \"alpha_bar_seq\": alpha_bar_seq,\n",
    "    }\n"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hfvtHEFf_7Q3",
    "ExecuteTime": {
     "end_time": "2025-06-11T16:19:36.831425Z",
     "start_time": "2025-06-11T16:19:36.819408Z"
    }
   },
   "source": [
    "def ddpm_forward(\n",
    "        unet: DenoisingUNet,\n",
    "        ddpm_schedule: dict,\n",
    "        x_0: torch.Tensor,\n",
    "        num_ts: int,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Algorithm 1 of the DDPM paper (not including gradient step).\n",
    "\n",
    "    Args:\n",
    "        unet: DenoisingUNet\n",
    "        ddpm_schedule: dict\n",
    "        x_0: (N, C, H, W) input tensor.\n",
    "        num_ts: int, number of timesteps.\n",
    "    Returns:\n",
    "        (,) diffusion loss.\n",
    "    \"\"\"\n",
    "    unet.train()\n",
    "    t = torch.randint(low=0, high=num_ts, size=(x_0.shape[0],),\n",
    "                      device=x_0.device)  # Uniform distribution for each sample\n",
    "    epsilon = torch.randn_like(x_0)  # Normal distribution with mean 0 and variance 1\n",
    "\n",
    "    alpha_bar_seq = ddpm_schedule[\"alpha_bar_seq\"]\n",
    "    alpha_bar_t = alpha_bar_seq[t].view(-1, 1, 1, 1)\n",
    "\n",
    "    x_t = torch.sqrt(alpha_bar_t) * x_0 + torch.sqrt(1 - alpha_bar_t) * epsilon\n",
    "    t_normalized = t.view(-1, 1).float() / num_ts  # normalize t to [0, 1]\n",
    "\n",
    "    epsilon_roof = unet(x_t, t_normalized)\n",
    "\n",
    "    return F.mse_loss(epsilon_roof, epsilon)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BNE8-455IDm3",
    "ExecuteTime": {
     "end_time": "2025-06-11T16:19:36.870667Z",
     "start_time": "2025-06-11T16:19:36.860245Z"
    }
   },
   "source": [
    "@torch.inference_mode()\n",
    "def ddpm_sample(\n",
    "        unet: DenoisingUNet,\n",
    "        ddpm_schedule: dict,\n",
    "        img_wh: tuple[int, int],\n",
    "        batch_size: int,\n",
    "        num_ts: int,\n",
    "        device: str = 'cuda'\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Algorithm 2 of the DDPM paper.\n",
    "\n",
    "    Args:\n",
    "        unet: DenoisingUNet\n",
    "        ddpm_schedule: dict\n",
    "        img_wh: (H, W) output image width and height.\n",
    "        num_ts: int, number of timesteps.\n",
    "\n",
    "    Returns:\n",
    "        (N, C, H, W) final sample.\n",
    "    \"\"\"\n",
    "    unet.eval()\n",
    "    x_t = torch.randn(batch_size, 1, img_wh[0], img_wh[1],\n",
    "                      device=device)  # Normal distribution with mean 0 and variance 1\n",
    "\n",
    "    beta_seq = ddpm_schedule[\"beta_seq\"]\n",
    "    alpha_seq = ddpm_schedule[\"alpha_seq\"]\n",
    "    alpha_bar_seq = ddpm_schedule[\"alpha_bar_seq\"]\n",
    "\n",
    "    for t in reversed(range(1, num_ts)):\n",
    "        t_idx = torch.full((batch_size,), t, device=device, dtype=torch.long)\n",
    "        t_normalized = t_idx.float().unsqueeze(1) / num_ts  # (N, 1)\n",
    "\n",
    "        # Schedule scalars for batch\n",
    "        beta_t = beta_seq[t_idx].view(-1, 1, 1, 1)\n",
    "        alpha_t = alpha_seq[t_idx].view(-1, 1, 1, 1)\n",
    "        alpha_bar_t = alpha_bar_seq[t_idx].view(-1, 1, 1, 1)\n",
    "\n",
    "        if t > 1:\n",
    "            alpha_bar_prev = alpha_seq[t_idx - 1].view(-1, 1, 1, 1)\n",
    "            z = torch.randn_like(x_t)\n",
    "        else:\n",
    "            alpha_bar_prev = torch.ones_like(alpha_bar_t)\n",
    "            z = torch.zeros_like(x_t)\n",
    "\n",
    "        epsilon_roof = unet(x_t, t_normalized)\n",
    "        x_hat0 = (x_t - torch.sqrt(1 - alpha_bar_t) * epsilon_roof) / torch.sqrt(alpha_bar_t)\n",
    "\n",
    "        x_t = (torch.sqrt(alpha_bar_prev) * beta_t / (1 - alpha_bar_t) * x_hat0 +\n",
    "               torch.sqrt(alpha_t) * (1 - alpha_bar_prev) / (1 - alpha_bar_t) * x_t +\n",
    "               torch.sqrt(beta_t) * z\n",
    "               )\n",
    "\n",
    "    return x_t\n"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "G_hVifFyw20j",
    "ExecuteTime": {
     "end_time": "2025-06-11T16:19:36.893134Z",
     "start_time": "2025-06-11T16:19:36.885135Z"
    }
   },
   "source": [
    "# Do Not Modify\n",
    "class DDPM(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            unet: DenoisingUNet,\n",
    "            betas: tuple[float, float] = (1e-4, 0.02),\n",
    "            num_ts: int = 300,\n",
    "            p_uncond: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.unet = unet\n",
    "        self.betas = betas\n",
    "        self.num_ts = num_ts\n",
    "        self.p_uncond = p_uncond\n",
    "        self.ddpm_schedule = ddpm_schedule(betas[0], betas[1], num_ts)\n",
    "\n",
    "        for k, v in ddpm_schedule(betas[0], betas[1], num_ts).items():\n",
    "            self.register_buffer(k, v, persistent=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (N, C, H, W) input tensor.\n",
    "\n",
    "        Returns:\n",
    "            (,) diffusion loss.\n",
    "        \"\"\"\n",
    "        return ddpm_forward(\n",
    "            self.unet, self.ddpm_schedule, x, self.num_ts\n",
    "        )\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def sample(\n",
    "            self,\n",
    "            img_wh: tuple[int, int],\n",
    "            batch_size: int\n",
    "    ):\n",
    "        return ddpm_sample(\n",
    "            self.unet, self.ddpm_schedule, img_wh, batch_size, self.num_ts\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 Train your denoiser"
   ],
   "metadata": {
    "id": "ACe_cr2_dv7I"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import random\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ──────────────────────────────── Hyper-parameters ──────────────────────────────\n",
    "run_name = \"uncond_ddpm_mnist\"\n",
    "num_hidden = 256\n",
    "batch_size = 128\n",
    "num_epochs = 30\n",
    "lr = 2e-4\n",
    "gamma = 0.1 ** (1.0 / num_epochs)  # exponential LR decay\n",
    "img_wh: Tuple[int, int] = (28, 28)\n",
    "eval_batch_size = 20\n",
    "T = 300  # diffusion steps\n",
    "checkpoint_dir = Path(\"checkpoints\")\n",
    "checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ──────────────────────────────── Data loaders ──────────────────────────────────\n",
    "train_data = MNIST(root=\"./data\", train=True, download=True, transform=ToTensor())\n",
    "test_data = MNIST(root=\"./data\", train=False, download=True, transform=ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "eval_loader = DataLoader(test_data, batch_size=eval_batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# ──────────────────────────────── Model & DDPM ──────────────────────────────────\n",
    "denoiser = DenoisingUNet(in_channels=1, num_hiddens=num_hidden)\n",
    "ddpm = DDPM(denoiser, num_ts=T).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(ddpm.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
    "\n",
    "# ──────────────────────────────── Tracking buffers ──────────────────────────────\n",
    "batch_losses = []\n",
    "epoch_losses = []\n",
    "\n",
    "# ──────────────────────────────── Training loop ─────────────────────────────────\n",
    "for epoch in range(num_epochs):\n",
    "    ddpm.train()\n",
    "    running = 0.0\n",
    "\n",
    "    pbar = tqdm(enumerate(train_loader, 1),\n",
    "                total=len(train_loader),\n",
    "                desc=f\"Epoch {epoch + 1:02d}/{num_epochs:02d}\")\n",
    "\n",
    "    for step, (data, _labels) in pbar:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = ddpm(data)  # forward & internally samples t\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        running += batch_loss\n",
    "        batch_losses.append(batch_loss)\n",
    "\n",
    "        pbar.set_postfix(loss=f\"{batch_loss:.4f}\")\n",
    "\n",
    "    avg_epoch_loss = running / len(train_loader)\n",
    "    epoch_losses.append(avg_epoch_loss)\n",
    "    print(f\"Epoch {epoch + 1:02d}: mean loss = {avg_epoch_loss:.4f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # ───────────────────────── evaluation ─────────────────────────\n",
    "    ddpm.eval()\n",
    "    with torch.no_grad():\n",
    "        noise_batch = next(iter(eval_loader))[0].to(device)  # just to get batch size/shape\n",
    "        samples = ddpm.sample(img_wh=img_wh,\n",
    "                              batch_size=eval_batch_size,\n",
    "                              device=device)\n",
    "\n",
    "    # grid & save\n",
    "    grid = make_grid(samples, nrow=int(eval_batch_size ** 0.5), normalize=True, value_range=(0, 1))\n",
    "    save_path = Path(f\"samples_epoch{epoch + 1:02d}.png\")\n",
    "    save_image(grid, save_path)\n",
    "    print(f\"Saved sample grid to {save_path}\")\n",
    "\n",
    "    # ───────────────────────── checkpoint ────────────────────────\n",
    "    random_id = random.randint(1000, 9999)\n",
    "    ckpt_name = f\"{run_name}-{random_id:04d}-epoch{epoch + 1:02d}.pt\"\n",
    "    torch.save({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"model_state\": ddpm.state_dict(),\n",
    "        \"opt_state\": optimizer.state_dict(),\n",
    "        \"scheduler_state\": scheduler.state_dict(),\n",
    "        \"batch_losses\": batch_losses,\n",
    "        \"epoch_losses\": epoch_losses,\n",
    "    }, checkpoint_dir / ckpt_name)\n",
    "    print(f\"Checkpoint saved as {ckpt_name}\")\n",
    "\n",
    "# ──────────────────────────────── Plot losses ───────────────────────────────────\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(batch_losses, label=\"batch loss\")\n",
    "plt.title(\"Batch loss\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.grid(True);\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epoch_losses, marker=\"o\", label=\"epoch loss\")\n",
    "plt.title(\"Epoch loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.grid(True);\n",
    "plt.legend()\n",
    "plt.savefig(\"loss_curves.png\")\n",
    "print(\"Loss curves saved to loss_curves.png\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "id": "VSChVRmJYO7L",
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-06-11T16:19:36.911228Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Epoch 01/30:   0%|          | 0/468 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "006d78931ac74b5e9ab22d124e91e16d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Implementing class-conditioned diffusion framework with CFG\n"
   ],
   "metadata": {
    "id": "uW2FBjpn8CTZ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###3.1 Adding Class-Conditioning to UNet architecture"
   ],
   "metadata": {
    "id": "irot7PI1I2Gi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class ConditionalDenoisingUNet(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels: int,\n",
    "            num_classes: int,\n",
    "            num_hiddens: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE.\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            x: torch.Tensor,\n",
    "            c: torch.Tensor,\n",
    "            t: torch.Tensor,\n",
    "            mask: torch.Tensor | None = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (N, C, H, W) input tensor.\n",
    "            c: (N, num_classes) float condition tensor.\n",
    "            t: (N, 1) normalized time tensor.\n",
    "            mask: (N, 1) mask tensor. If not None, mask out condition when mask == 0.\n",
    "\n",
    "        Returns:\n",
    "            (N, C, H, W) output tensor.\n",
    "        \"\"\"\n",
    "        assert x.shape[-2:] == (28, 28), \"Expect input shape to be (28, 28).\"\n",
    "        # YOUR CODE HERE.\n",
    "        raise NotImplementedError()"
   ],
   "metadata": {
    "id": "vAXZYlOt8Rzy"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "###3.2 DDPM Forward and Inverse Process with CFG"
   ],
   "metadata": {
    "id": "uV3lTJz8IxrE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def ddpm_forward(\n",
    "        unet: ConditionalDenoisingUNet,\n",
    "        ddpm_schedule: dict,\n",
    "        x_0: torch.Tensor,\n",
    "        c: torch.Tensor,\n",
    "        p_uncond: float,\n",
    "        num_ts: int,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Algorithm 3 (not including gradient step).\n",
    "\n",
    "    Args:\n",
    "        unet: ConditionalDenoisingUNet\n",
    "        ddpm_schedule: dict\n",
    "        x_0: (N, C, H, W) input tensor.\n",
    "        c: (N,) int64 condition tensor.\n",
    "        p_uncond: float, probability of unconditioning the condition.\n",
    "        num_ts: int, number of timesteps.\n",
    "\n",
    "    Returns:\n",
    "        (,) diffusion loss.\n",
    "    \"\"\"\n",
    "    unet.train()\n",
    "    # YOUR CODE HERE.\n",
    "    raise NotImplementedError()"
   ],
   "metadata": {
    "id": "NobmVh4U8BRP"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "@torch.inference_mode()\n",
    "def ddpm_cfg_sample(\n",
    "        unet: ConditionalDenoisingUNet,\n",
    "        ddpm_schedule: dict,\n",
    "        c: torch.Tensor,\n",
    "        img_wh: tuple[int, int],\n",
    "        num_ts: int,\n",
    "        guidance_scale: float = 5.0\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Algorithm 4.\n",
    "\n",
    "    Args:\n",
    "        unet: ConditionalDenoisingUNet\n",
    "        ddpm_schedule: dict\n",
    "        c: (N,) int64 condition tensor. Only for class-conditional\n",
    "        img_wh: (H, W) output image width and height.\n",
    "        num_ts: int, number of timesteps.\n",
    "        guidance_scale: float, CFG scale.\n",
    "\n",
    "    Returns:\n",
    "        (N, C, H, W) final sample.\n",
    "    \"\"\"\n",
    "    unet.eval()\n",
    "    # YOUR CODE HERE.\n",
    "    raise NotImplementedError()"
   ],
   "metadata": {
    "id": "rMW5YeCi8cqO"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Do Not Modify\n",
    "class DDPM(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            unet: ConditionalDenoisingUNet,\n",
    "            betas: tuple[float, float] = (1e-4, 0.02),\n",
    "            num_ts: int = 300,\n",
    "            p_uncond: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.unet = unet\n",
    "        self.betas = betas\n",
    "        self.num_ts = num_ts\n",
    "        self.p_uncond = p_uncond\n",
    "        self.ddpm_schedule = ddpm_schedule(betas[0], betas[1], num_ts)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, c: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (N, C, H, W) input tensor.\n",
    "            c: (N,) int64 condition tensor.\n",
    "\n",
    "        Returns:\n",
    "            (,) diffusion loss.\n",
    "        \"\"\"\n",
    "        return ddpm_forward(\n",
    "            self.unet, self.ddpm_schedule, x, c, self.p_uncond, self.num_ts\n",
    "        )\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def sample(\n",
    "            self,\n",
    "            c: torch.Tensor,\n",
    "            img_wh: tuple[int, int],\n",
    "            guidance_scale: float = 5.0\n",
    "    ):\n",
    "        return ddpm_cfg_sample(\n",
    "            self.unet, self.ddpm_schedule, c, img_wh, self.num_ts, guidance_scale\n",
    "        )"
   ],
   "metadata": {
    "id": "gdQFWwIt8mXh"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "###3.3 Train your class-conditioned denoiser"
   ],
   "metadata": {
    "id": "EEGqlFNClOaw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# YOUR CODE HERE."
   ],
   "metadata": {
    "id": "MkAIikcEMFEm"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "###3.4 Experiment with different guidance sacles"
   ],
   "metadata": {
    "id": "m4iTw-TFGYhA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# YOUR CODE HERE.\n"
   ],
   "metadata": {
    "id": "d9gnGqPOoXT2"
   },
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "ryMrLOORbWLz",
    "k1iFNqV8HjzM",
    "GCawfhu0HqcH",
    "t9JhNQN5Ad3V",
    "0nyxOM-RbZnC",
    "m4iTw-TFGYhA"
   ],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kR0SKBDbaOqR"
   },
   "source": [
    "# Neural Graphics Ex1: Training Your Own Diffusion Model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryMrLOORbWLz"
   },
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lX3XpcGSXBIs",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5dc23e69-96b7-4722-d842-dbc108ce5522"
   },
   "source": [
    "# We recommend using these utils.\n",
    "# https://google.github.io/mediapy/mediapy.html\n",
    "# https://einops.rocks/\n",
    "# !pip install mediapy einops --quiet"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VdFQ6c9-Pm4Y"
   },
   "source": [
    "# Import essential modules. Feel free to add whatever you need.\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from random import randint\n",
    "import numpy as np\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Seed your work\n",
    "To be able to reproduce your code, please use a random seed from this point onward."
   ],
   "metadata": {
    "id": "pDJ3QzHdRV52"
   }
  },
  {
   "metadata": {
    "id": "aDVpoyjaRcoC"
   },
   "cell_type": "code",
   "source": [
    "def seed_everything(seed):\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "YOUR_SEED = 180  # modify if you want\n",
    "seed_everything(YOUR_SEED)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dokvxybn_DwK"
   },
   "source": [
    "## 1. Basic Ops and UNet blocks\n",
    "**Notations:**  \n",
    " * `Conv2D(kernel_size, stride, padding)` is `nn.Conv2d()`  \n",
    " * `BN` is `nn.BatchNorm2d()`  \n",
    " * `GELU` is `nn.GELU()`  \n",
    " * `ConvTranspose2D(kernel_size, stride, padding)` is `nn.ConvTranspose2d()`  \n",
    " * `AvgPool(kernel_size)` is `nn.AvgPool2d()`  \n",
    " * `Linear` is `nn.Linear()`  \n",
    " * `N`, `C`, `W` and `H` are batch size, channels num, weight and height respectively\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Basic Ops"
   ],
   "metadata": {
    "id": "k1iFNqV8HjzM"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fhpEzgwCJqbW"
   },
   "source": [
    "class Conv(nn.Module):\n",
    "    \"\"\"\n",
    "    A convolutional layer that doesnâ€™t change the image\n",
    "    resolution, only the channel dimension\n",
    "    Applies nn.Conv2d(3, 1, 1) followed by BN and GELU.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        \"\"\"\n",
    "        Initializes the Conv layer\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels\n",
    "            out_channels (int): The number of output channels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv2d = nn.Conv2d(kernel_size=3,\n",
    "                                in_channels=in_channels,\n",
    "                                out_channels=out_channels,\n",
    "                                padding=1,\n",
    "                                stride=1, )\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (N, in_channels, H, W) input tensor.\n",
    "\n",
    "        Returns:\n",
    "            (N, out_channels, H, W) output tensor.\n",
    "        \"\"\"\n",
    "        x = self.conv2d(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.gelu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DownConv(nn.Module):\n",
    "    \"\"\"\n",
    "        A convolutional layer down-samples the tensor by 2.\n",
    "        The layer consists of Conv2D(3, 2, 1) followed by BN and GELU.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        \"\"\"\n",
    "        Initializes the DownConv layer\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels\n",
    "            out_channels (int): The number of output channels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv2d = nn.Conv2d(kernel_size=3,\n",
    "                                in_channels=in_channels,\n",
    "                                out_channels=out_channels,\n",
    "                                stride=2,\n",
    "                                padding=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (N, in_channels, H, W) input tensor.\n",
    "\n",
    "        Returns:\n",
    "            (N, out_channels, H/2, W/2) output tensor.\n",
    "        \"\"\"\n",
    "        x = self.conv2d(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.gelu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UpConv(nn.Module):\n",
    "    \"\"\"\n",
    "    A convolutional layer that upsamples the tensor by 2.\n",
    "    The layer consists of ConvTranspose2d(4, 2, 1) followed by\n",
    "    BN and GELU.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        \"\"\"\n",
    "        Initializes the UpConv layer\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels\n",
    "            out_channels (int): The number of output channels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.convTranspose2d = nn.ConvTranspose2d(kernel_size=2,\n",
    "                                                  in_channels=in_channels,\n",
    "                                                  out_channels=out_channels,\n",
    "                                                  stride=2,\n",
    "                                                  padding=0\n",
    "                                                  )\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (N, in_channels, H, W) input tensor.\n",
    "\n",
    "        Returns:\n",
    "            (N, out_channels, H*2, W*2) output tensor.\n",
    "        \"\"\"\n",
    "        x = self.convTranspose2d(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.gelu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    \"\"\"\n",
    "    Average pooling layer that flattens a 7x7 tensor into a 1x1 tensor.\n",
    "    The layer consists of AvgPool followed by GELU.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.avgpool = nn.AvgPool2d(7)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (N, C, 7, 7) input tensor.\n",
    "\n",
    "        Returns:\n",
    "            (N, C, 1, 1) output tensor.\n",
    "        \"\"\"\n",
    "        x = self.avgpool(x)\n",
    "        x = self.gelu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Unflatten(nn.Module):\n",
    "    \"\"\"\n",
    "      Convolutional layer that expends/up-samples a 1x1 tensor into a\n",
    "      7x7 tensor. The layer consists of ConvTranspose2D(7, 7, 0)\n",
    "      followed by BN and GELU.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int):\n",
    "        \"\"\"\n",
    "        Initializes Unflatten layer\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.convtranspose2d = nn.ConvTranspose2d(kernel_size=7,\n",
    "                                                  stride=7,\n",
    "                                                  padding=0,\n",
    "                                                  in_channels=in_channels,\n",
    "                                                  out_channels=in_channels)\n",
    "        self.bn = nn.BatchNorm2d(in_channels)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (N, in_channels, 1, 1) input tensor.\n",
    "\n",
    "        Returns:\n",
    "            (N, in_channels, 7, 7) output tensor.\n",
    "        \"\"\"\n",
    "        x = self.convtranspose2d(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.gelu(x)\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### UNet Blocks"
   ],
   "metadata": {
    "id": "GCawfhu0HqcH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Two consecutive Conv operations.\n",
    "    Note that it has the same input and output shape as Conv.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        \"\"\"\n",
    "        Initializes ConvBlock\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels\n",
    "            out_channels (int): The number of output channels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv(in_channels, out_channels)\n",
    "        self.conv2 = Conv(out_channels, out_channels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (N, in_channels, H, W) input tensor.\n",
    "\n",
    "        Returns:\n",
    "            (N, out_channels, H, W) output tensor.\n",
    "        \"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    DownConv followed by ConvBlock. Note that it has the same input and output\n",
    "    shape as DownConv.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        \"\"\"\n",
    "        Initializes DownBlock\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels\n",
    "            out_channels (int): The number of output channels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv1 = DownConv(in_channels, out_channels)\n",
    "        self.conv2 = ConvBlock(out_channels, out_channels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (N, in_channels, H, W) input tensor.\n",
    "\n",
    "        Returns:\n",
    "            (N, out_channels, H/2, W/2) output tensor.\n",
    "        \"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    UpConv followed by ConvBlock.\n",
    "    Note that it has the same input and output shape as UpConv\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        \"\"\"\n",
    "        Initializes UpBlock\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels\n",
    "            out_channels (int): The number of output channels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv1 = UpConv(in_channels, out_channels)\n",
    "        self.conv2 = ConvBlock(out_channels, out_channels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (N, in_channels, H, W) input tensor.\n",
    "\n",
    "        Returns:\n",
    "            (N, out_channels, H*2, W*2) output tensor.\n",
    "        \"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FCBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Fully-connected Block, consisting of FC layer followed by Linear layer. Note\n",
    "    that it has the same input and output shape as FC.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        \"\"\"\n",
    "        Initializes FCBlock\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels\n",
    "            out_channels (int): The number of output channels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.fc = FC(in_channels, out_channels)\n",
    "        self.linear = nn.Linear(out_channels, out_channels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (N, in_channels) input tensor.\n",
    "\n",
    "        Returns:\n",
    "            (N, out_channels) output tensor.\n",
    "        \"\"\"\n",
    "        x = self.fc(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FC(nn.Module):\n",
    "    \"\"\"\n",
    "    Fully connected layer, consisting of nn.linear followed by GELU.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        \"\"\"\n",
    "        Initializes the FC layer\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels\n",
    "            out_channels (int): The number of output channels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_channels, out_channels)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (N, in_channels) input tensor.\n",
    "\n",
    "        Returns:\n",
    "            (N, out_channels) output tensor.\n",
    "        \"\"\"\n",
    "        x = self.fc(x)\n",
    "        x = self.gelu(x)\n",
    "        return x"
   ],
   "metadata": {
    "id": "3nusVuFlHt67"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMI3IMkjayxQ"
   },
   "source": [
    "## 2. Unconditional Diffusion Framework\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 UNet architecture"
   ],
   "metadata": {
    "id": "t9JhNQN5Ad3V"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fkchbyYkzAvV"
   },
   "source": [
    "class DenoisingUNet(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels: int,  # 1\n",
    "            num_hiddens: int  # D\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # t-tensor\n",
    "        self.t_fc4 = FCBlock(in_channels, 2 * num_hiddens)\n",
    "        self.t_fc2 = FCBlock(in_channels, num_hiddens)\n",
    "\n",
    "        # In\n",
    "        self.conv_block_in = ConvBlock(in_channels, num_hiddens)  # (N, D, 28, 28)\n",
    "        # Down\n",
    "        self.down_block1 = DownBlock(num_hiddens, num_hiddens)  # (N, D, 14, 14)\n",
    "        self.down_block2 = DownBlock(num_hiddens, 2 * num_hiddens)  # (N, 2 * D, 7, 7)\n",
    "        self.flatten = Flatten()  # (N, 2 * D, 1, 1)\n",
    "        # Up / with skip connections and t-tensor addition\n",
    "        self.expend = Unflatten(2 * num_hiddens)  # (N, [2 * D + 2 * D], 7, 7)\n",
    "        self.up_block2 = UpBlock(4 * num_hiddens, num_hiddens)  # (N, [D + D], 14, 14)\n",
    "        self.up_block1 = UpBlock(2 * num_hiddens, num_hiddens)  # (N, [D + D], 28, 28)\n",
    "        # Out\n",
    "        self.conv_block_out = ConvBlock(2 * num_hiddens, num_hiddens)  # (N, D, 28, 28)\n",
    "        self.conv2d = nn.Conv2d(\n",
    "            in_channels=num_hiddens,\n",
    "            out_channels=1,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            x: torch.Tensor,\n",
    "            t: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (N, C, H, W) input tensor.\n",
    "            t: (N, 1) normalized time tensor.\n",
    "\n",
    "        Returns:\n",
    "            (N, C, H, W) output tensor.\n",
    "        \"\"\"\n",
    "        assert x.shape[-2:] == (28, 28), \"Expect input shape to be (28, 28).\"\n",
    "        # t-tensor\n",
    "        t_layer4 = self.t_fc4(t).unsqueeze(-1).unsqueeze(-1)\n",
    "        t_layer2 = self.t_fc2(t).unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        # Down\n",
    "        d_layer1 = self.conv_block_in(x)\n",
    "        d_layer2 = self.down_block1(d_layer1)\n",
    "        d_layer3 = self.down_block2(d_layer2)\n",
    "\n",
    "        # Flatten/Extract\n",
    "        flat = self.flatten(d_layer3)\n",
    "        expend = self.expend(flat)\n",
    "\n",
    "        expend = expend + t_layer4  # t-tensor addition layer 4\n",
    "\n",
    "        # Up\n",
    "        u_layer2 = self.up_block2(torch.cat((d_layer3, expend), dim=1))  # skip connection layer 3\n",
    "        u_layer2 += t_layer2  # t-tensor addition layer 2\n",
    "\n",
    "        u_layer1 = self.up_block1(torch.cat((d_layer2, u_layer2), dim=1))  # skip connection layer 2\n",
    "        output = self.conv_block_out(torch.cat((d_layer1, u_layer1), dim=1))  # skip connection layer 1\n",
    "        output = self.conv2d(output)\n",
    "        return output"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nyxOM-RbZnC"
   },
   "source": "### 2.2 DDPM Forward and Inverse Process\n"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yIvMw63T6JkE"
   },
   "source": [
    "def ddpm_schedule(beta1: float, beta2: float, num_ts: int, device: str = 'cuda') -> dict:\n",
    "    \"\"\"Constants for DDPM training and sampling.\n",
    "\n",
    "    Arguments:\n",
    "        beta1: float, starting beta value.\n",
    "        beta2: float, ending beta value.\n",
    "        num_ts: int, number of timesteps.\n",
    "\n",
    "    Returns:\n",
    "        dict with keys:\n",
    "            betas: linear schedule of betas from beta1 to beta2.\n",
    "            alphas: 1 - betas.\n",
    "            alpha_bars: cumulative product of alphas.\n",
    "            device: cuda or cpu\n",
    "    \"\"\"\n",
    "    assert beta1 < beta2 < 1.0, \"Expect beta1 < beta2 < 1.0.\"\n",
    "    beta_list = torch.linspace(beta1, beta2, num_ts, device=device)\n",
    "    alpha_list = 1 - beta_list\n",
    "    alpha_bar_list = torch.cumprod(alpha_list, dim=0)\n",
    "    alpha_bar_list = torch.cat([torch.ones(1, device=device), alpha_bar_list])\n",
    "\n",
    "    return {'beta_list': beta_list, 'alpha_list': alpha_list, 'alpha_bar_list': alpha_bar_list}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hfvtHEFf_7Q3"
   },
   "source": [
    "def ddpm_forward(\n",
    "        unet: DenoisingUNet,\n",
    "        ddpm_schedule: dict,\n",
    "        x_0: torch.Tensor,\n",
    "        num_ts: int,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Algorithm 1 of the DDPM paper (not including gradient step).\n",
    "\n",
    "    Args:\n",
    "        unet: DenoisingUNet\n",
    "        ddpm_schedule: dict\n",
    "        x_0: (N, C, H, W) input tensor.\n",
    "        num_ts: int, number of timesteps.\n",
    "    Returns:\n",
    "        (,) diffusion loss.\n",
    "    \"\"\"\n",
    "    unet.train()\n",
    "    N = x_0.shape[0]\n",
    "\n",
    "    t = torch.randint(low=1, high=num_ts + 1, size=(N,), device=x_0.device)  # t ~ Uniform({1, ..., T})\n",
    "\n",
    "    alpha_bar_t = ddpm_schedule['alpha_bar_list'][t].view(-1, 1, 1, 1)\n",
    "\n",
    "    e = torch.randn_like(x_0)  # e ~ N(0, I)\n",
    "\n",
    "    x_t = torch.sqrt(alpha_bar_t) * x_0 + torch.sqrt(1 - alpha_bar_t) * e\n",
    "\n",
    "    t_normalized = t.view(-1, 1).float().to(x_t.device) / num_ts\n",
    "\n",
    "    e_hat = unet(x_t, t_normalized)\n",
    "\n",
    "    return ((e - e_hat) ** 2).mean()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BNE8-455IDm3"
   },
   "source": [
    "@torch.inference_mode()\n",
    "def ddpm_sample(\n",
    "        unet: DenoisingUNet,\n",
    "        ddpm_schedule: dict,\n",
    "        img_wh: tuple[int, int],\n",
    "        batch_size: int,\n",
    "        num_ts: int\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Algorithm 2 of the DDPM paper.\n",
    "\n",
    "    Args:\n",
    "        unet: DenoisingUNet\n",
    "        ddpm_schedule: dict\n",
    "        img_wh: (H, W) output image width and height.\n",
    "        num_ts: int, number of timesteps.\n",
    "\n",
    "    Returns:\n",
    "        (N, C, H, W) final sample.\n",
    "    \"\"\"\n",
    "    unet.eval()\n",
    "\n",
    "    betas = ddpm_schedule['beta_list']\n",
    "    alphas = ddpm_schedule['alpha_list']\n",
    "    alpha_bar = ddpm_schedule['alpha_bar_list']\n",
    "\n",
    "    x_t = torch.randn(batch_size, 1, img_wh[0], img_wh[1], device=betas.device)  # x_t ~ N(0, I)\n",
    "\n",
    "    for t in range(num_ts, 0, -1):\n",
    "\n",
    "        b_t = betas[t - 1].view(1, 1, 1, 1)\n",
    "        a_t = alphas[t - 1].view(1, 1, 1, 1)\n",
    "        a_b_t = alpha_bar[t].view(1, 1, 1, 1)\n",
    "        a_b_t_m_1 = alpha_bar[t - 1].view(1, 1, 1, 1)\n",
    "\n",
    "        t_normalized = torch.full((batch_size, 1), t / num_ts, device=betas.device)\n",
    "        e_hat = unet(x_t, t_normalized)\n",
    "\n",
    "        if t > 1:\n",
    "            z = torch.randn_like(x_t)\n",
    "        else:\n",
    "            z = torch.zeros_like(x_t)\n",
    "\n",
    "        x_0_hat = (1 / torch.sqrt(a_b_t)) * (x_t - torch.sqrt(1 - a_b_t) * e_hat)\n",
    "\n",
    "        cof1 = (torch.sqrt(a_b_t_m_1) * b_t) / (1 - a_b_t)\n",
    "        cof2 = (torch.sqrt(a_t) * (1 - a_b_t_m_1)) / (1 - a_b_t)\n",
    "        cof3 = torch.sqrt(b_t)\n",
    "\n",
    "        x_t = cof1 * x_0_hat + cof2 * x_t + cof3 * z\n",
    "\n",
    "    return x_t\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "G_hVifFyw20j"
   },
   "source": [
    "# Do Not Modify\n",
    "class DDPM(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            unet: DenoisingUNet,\n",
    "            betas: tuple[float, float] = (1e-4, 0.02),\n",
    "            num_ts: int = 300,\n",
    "            p_uncond: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.unet = unet\n",
    "        self.betas = betas\n",
    "        self.num_ts = num_ts\n",
    "        self.p_uncond = p_uncond\n",
    "        self.ddpm_schedule = ddpm_schedule(betas[0], betas[1], num_ts)\n",
    "\n",
    "        for k, v in ddpm_schedule(betas[0], betas[1], num_ts).items():\n",
    "            self.register_buffer(k, v, persistent=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (N, C, H, W) input tensor.\n",
    "\n",
    "        Returns:\n",
    "            (,) diffusion loss.\n",
    "        \"\"\"\n",
    "        return ddpm_forward(\n",
    "            self.unet, self.ddpm_schedule, x, self.num_ts\n",
    "        )\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def sample(\n",
    "            self,\n",
    "            img_wh: tuple[int, int],\n",
    "            batch_size: int\n",
    "    ):\n",
    "        return ddpm_sample(\n",
    "            self.unet, self.ddpm_schedule, img_wh, batch_size, self.num_ts\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 Train your denoiser"
   ],
   "metadata": {
    "id": "ACe_cr2_dv7I"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Hyper parameters - Modify if you wish\n",
    "num_hidden = 128\n",
    "batch_size = 64\n",
    "num_epochs = 20\n",
    "lr = 1e-3\n",
    "img_wh = (28, 28)\n",
    "eval_batch_size = 20\n",
    "T = 300\n",
    "\n",
    "train_data = MNIST(root='./data', train=True, download=True, transform=transforms.Compose([\n",
    "    ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "]))\n",
    "test_data = MNIST(root='./data', train=False, download=True, transform=transforms.Compose([\n",
    "    ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "]))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "eval_loader = DataLoader(test_data, batch_size=eval_batch_size,\n",
    "                         shuffle=True)  # Not usefull now, but will be for evaluating class-conditioned denoiser (3.3)\n",
    "\n",
    "# Init denoiser and DDPM wrapper\n",
    "denosier_unet = DenoisingUNet(in_channels=1, num_hiddens=num_hidden)\n",
    "ddpm = DDPM(denosier_unet, num_ts=T)\n",
    "\n",
    "# Optimizer and device setup - Adam optimizer with exponential learning rate decay\n",
    "optimizer = torch.optim.Adam(ddpm.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.1 ** (1.0 / num_epochs))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "ddpm.to(device)\n",
    "\n",
    "train_batch_losses = []\n",
    "train_epoch_losses = []\n",
    "\n",
    "run_id = f\"{randint(0, 9999):04d}\"\n",
    "base_dir = os.path.join('runs', run_id)\n",
    "plot_dir = os.path.join(base_dir, 'plots')\n",
    "weight_dir = os.path.join(base_dir, 'weights')\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "os.makedirs(weight_dir, exist_ok=True)\n",
    "\n",
    "tqdm.write(f\"start training {run_id} on device: {device}\")\n",
    "\n",
    "total_iters = num_epochs * len(train_loader)  # single progress bar for the whole run\n",
    "\n",
    "try:\n",
    "    pbar = tqdm(total=total_iters, desc='Training', dynamic_ncols=True)\n",
    "    for epoch in range(num_epochs):\n",
    "        ddpm.train()  # Set the model to training mode\n",
    "        epoch_loss = 0.0\n",
    "        for batch, (data, label) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            data = data.to(device)\n",
    "            loss = ddpm(data)\n",
    "            loss.backward()  # Compute gradients\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            batch_loss = loss.item()\n",
    "            train_batch_losses.append(batch_loss)\n",
    "            epoch_loss += batch_loss\n",
    "\n",
    "            pbar.set_description(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "            pbar.set_postfix(loss=f\"{batch_loss:.4f}\")\n",
    "            pbar.update(1)\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "        train_epoch_losses.append(avg_epoch_loss)\n",
    "        scheduler.step()\n",
    "\n",
    "        ddpm.eval()\n",
    "        with torch.no_grad():\n",
    "            samples = ddpm.sample(img_wh, eval_batch_size).cpu()\n",
    "        grid = torchvision.utils.make_grid(samples, normalize=True, value_range=(-1, 1))\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(grid.permute(1, 2, 0).cpu(), cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Samples at Epoch {epoch + 1}')\n",
    "        plt.savefig(os.path.join(plot_dir, f'samples_epoch_{epoch + 1:02d}.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        torch.save(ddpm.state_dict(), os.path.join(weight_dir, f'ddpm_epoch_{epoch + 1:02d}.pt'))\n",
    "except KeyboardInterrupt:\n",
    "    tqdm.write(\"Training interrupted by user.\")\n",
    "finally:\n",
    "    pbar.close()\n",
    "\n",
    "    # --------- Plots ---------\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(train_batch_losses, alpha=0.8, label='Batch loss')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('MSE loss')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plot_dir, 'batch_loss.png'), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(train_epoch_losses, linewidth=2, label='Epoch loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE loss')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plot_dir, 'epoch_loss.png'), dpi=300)\n",
    "    plt.close()\n"
   ],
   "metadata": {
    "id": "VSChVRmJYO7L"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Implementing class-conditioned diffusion framework with CFG\n"
   ],
   "metadata": {
    "id": "uW2FBjpn8CTZ"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### 3.1 Adding Class-Conditioning to UNet architecture",
   "metadata": {
    "id": "irot7PI1I2Gi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class ConditionalDenoisingUNet(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels: int,\n",
    "            num_classes: int,\n",
    "            num_hiddens: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        # t-tensor\n",
    "        self.t_fc4 = FCBlock(in_channels, 2 * num_hiddens)\n",
    "        self.t_fc2 = FCBlock(in_channels, num_hiddens)\n",
    "        # c-tesnor\n",
    "        self.c_fc4 = FCBlock(num_classes, 2 * num_hiddens)\n",
    "        self.c_fc2 = FCBlock(num_classes, num_hiddens)\n",
    "\n",
    "        # In\n",
    "        self.conv_block_in = ConvBlock(in_channels, num_hiddens)  # (N, D, 28, 28)\n",
    "        # Down\n",
    "        self.down_block1 = DownBlock(num_hiddens, num_hiddens)  # (N, D, 14, 14)\n",
    "        self.down_block2 = DownBlock(num_hiddens, 2 * num_hiddens)  # (N, 2 * D, 7, 7)\n",
    "        self.flatten = Flatten()  # (N, 2 * D, 1, 1)\n",
    "        # Up / with skip connections and t-tensor addition\n",
    "        self.expend = Unflatten(2 * num_hiddens)  # (N, [2 * D + 2 * D], 7, 7)\n",
    "        self.up_block2 = UpBlock(4 * num_hiddens, num_hiddens)  # (N, [D + D], 14, 14)\n",
    "        self.up_block1 = UpBlock(2 * num_hiddens, num_hiddens)  # (N, [D + D], 28, 28)\n",
    "        # Out\n",
    "        self.conv_block_out = ConvBlock(2 * num_hiddens, num_hiddens)  # (N, D, 28, 28)\n",
    "        self.conv2d = nn.Conv2d(\n",
    "            in_channels=num_hiddens,\n",
    "            out_channels=1,\n",
    "            kernel_size=3,  # or whatever you used earlier\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            x: torch.Tensor,\n",
    "            c: torch.Tensor,\n",
    "            t: torch.Tensor,\n",
    "            mask: torch.Tensor | None = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (N, C, H, W) input tensor.\n",
    "            c: (N, num_classes) float condition tensor.\n",
    "            t: (N, 1) normalized time tensor.\n",
    "            mask: (N, 1) mask tensor. If not None, mask out condition when mask == 0.\n",
    "\n",
    "        Returns:\n",
    "            (N, C, H, W) output tensor.\n",
    "        \"\"\"\n",
    "        assert x.shape[-2:] == (28, 28), \"Expect input shape to be (28, 28).\"\n",
    "        # t-tensor\n",
    "        t_layer4 = self.t_fc4(t).unsqueeze(-1).unsqueeze(-1)\n",
    "        t_layer2 = self.t_fc2(t).unsqueeze(-1).unsqueeze(-1)\n",
    "        # c-tensor\n",
    "        c_layer4 = self.c_fc4(c).unsqueeze(-1).unsqueeze(-1)\n",
    "        c_layer2 = self.c_fc2(c).unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        # Down\n",
    "        d_layer1 = self.conv_block_in(x)\n",
    "        d_layer2 = self.down_block1(d_layer1)\n",
    "        d_layer3 = self.down_block2(d_layer2)\n",
    "\n",
    "        # Flatten/Extract\n",
    "        flat = self.flatten(d_layer3)\n",
    "        expend = self.expend(flat)\n",
    "\n",
    "        expend = c_layer4 * expend + t_layer4  # c-tensor multiplication, t-tensor addition layer 4\n",
    "\n",
    "        # Up\n",
    "        u_layer2 = self.up_block2(torch.cat((d_layer3, expend), dim=1))  # skip connection layer 3\n",
    "        u_layer2 = c_layer2 * u_layer2 + t_layer2  # c-tensor multiplication, t-tensor addition layer 2\n",
    "\n",
    "        u_layer1 = self.up_block1(torch.cat((d_layer2, u_layer2), dim=1))  # skip connection layer 2\n",
    "        output = self.conv_block_out(torch.cat((d_layer1, u_layer1), dim=1))  # skip connection layer 1\n",
    "        output = self.conv2d(output)\n",
    "        return output"
   ],
   "metadata": {
    "id": "vAXZYlOt8Rzy"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### 3.2 DDPM Forward and Inverse Process with CFG",
   "metadata": {
    "id": "uV3lTJz8IxrE"
   }
  },
  {
   "metadata": {
    "id": "NobmVh4U8BRP"
   },
   "cell_type": "code",
   "source": [
    "def ddpm_forward(\n",
    "        unet: ConditionalDenoisingUNet,\n",
    "        ddpm_schedule: dict,\n",
    "        x_0: torch.Tensor,\n",
    "        c: torch.Tensor,\n",
    "        p_uncond: float,\n",
    "        num_ts: int,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Algorithm 3 (not including gradient step).\n",
    "\n",
    "    Args:\n",
    "        unet: ConditionalDenoisingUNet\n",
    "        ddpm_schedule: dict\n",
    "        x_0: (N, C, H, W) input tensor.\n",
    "        c: (N,) int64 condition tensor.\n",
    "        p_uncond: float, probability of unconditioning the condition.\n",
    "        num_ts: int, number of timesteps.\n",
    "\n",
    "    Returns:\n",
    "        (,) diffusion loss.\n",
    "    \"\"\"\n",
    "    unet.train()\n",
    "    N = x_0.shape[0]\n",
    "    num_classes = unet.num_classes\n",
    "\n",
    "    # make C into one-hot vector and set 0 with probability of p_uncond\n",
    "    c_one_hot = torch.eye(num_classes, device=c.device)[c]\n",
    "    c_one_hot *= torch.rand(N, 1, device=c.device) > p_uncond\n",
    "\n",
    "    t = torch.randint(low=1, high=num_ts + 1, size=(N,), device=x_0.device)  # t ~ Uniform({1, ..., T})\n",
    "\n",
    "    alpha_bar_t = ddpm_schedule['alpha_bar_list'][t].view(-1, 1, 1, 1)\n",
    "\n",
    "    e = torch.randn_like(x_0)  # e ~ N(0, I)\n",
    "\n",
    "    x_t = torch.sqrt(alpha_bar_t) * x_0 + torch.sqrt(1 - alpha_bar_t) * e\n",
    "\n",
    "    t_normalized = t.view(-1, 1).float().to(x_t.device) / num_ts\n",
    "\n",
    "    e_hat = unet(x_t, c_one_hot, t_normalized)\n",
    "\n",
    "    return ((e - e_hat) ** 2).mean()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "rMW5YeCi8cqO"
   },
   "cell_type": "code",
   "source": [
    "@torch.inference_mode()\n",
    "def ddpm_cfg_sample(\n",
    "        unet: ConditionalDenoisingUNet,\n",
    "        ddpm_schedule: dict,\n",
    "        c: torch.Tensor,\n",
    "        img_wh: tuple[int, int],\n",
    "        num_ts: int,\n",
    "        guidance_scale: float = 5.0\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Algorithm 4.\n",
    "\n",
    "    Args:\n",
    "        unet: ConditionalDenoisingUNet\n",
    "        ddpm_schedule: dict\n",
    "        c: (N,) int64 condition tensor. Only for class-conditional\n",
    "        img_wh: (H, W) output image width and height.\n",
    "        num_ts: int, number of timesteps.\n",
    "        guidance_scale: float, CFG scale.\n",
    "\n",
    "    Returns:\n",
    "        (N, C, H, W) final sample.\n",
    "    \"\"\"\n",
    "    unet.eval()\n",
    "    device = c.device\n",
    "    N = c.shape[0]\n",
    "    num_classes = unet.num_classes\n",
    "\n",
    "    # make C into one-hot vector\n",
    "    c_one_hot = torch.eye(num_classes, device=device)[c]\n",
    "\n",
    "    betas = ddpm_schedule['beta_list'].to(device)\n",
    "    alphas = ddpm_schedule['alpha_list'].to(device)\n",
    "    alpha_bar = ddpm_schedule['alpha_bar_list'].to(device)\n",
    "\n",
    "    x_t = torch.randn(N, 1, img_wh[0], img_wh[1], device=device)  # x_t ~ N(0, I)\n",
    "\n",
    "    for t in range(num_ts, 0, -1):\n",
    "        b_t = betas[t - 1].view(1, 1, 1, 1)\n",
    "        a_t = alphas[t - 1].view(1, 1, 1, 1)\n",
    "        a_b_t = alpha_bar[t].view(1, 1, 1, 1)\n",
    "        a_b_t_m_1 = alpha_bar[t - 1].view(1, 1, 1, 1)\n",
    "\n",
    "        t_normalized = torch.full((N, 1), t / num_ts, device=device)\n",
    "        e_cond = unet(x_t, c_one_hot, t_normalized)\n",
    "        e_uncond = unet(x_t, torch.zeros_like(c_one_hot, device=device), t_normalized)\n",
    "\n",
    "        e_hat = e_uncond + guidance_scale * (e_cond - e_uncond)\n",
    "\n",
    "        z = torch.randn_like(x_t) if t > 1 else torch.zeros_like(x_t)\n",
    "\n",
    "        x_0_hat = (1 / torch.sqrt(a_b_t)) * (x_t - torch.sqrt(1 - a_b_t) * e_hat)\n",
    "\n",
    "        cof1 = (torch.sqrt(a_b_t_m_1) * b_t) / (1 - a_b_t)\n",
    "        cof2 = (torch.sqrt(a_t) * (1 - a_b_t_m_1)) / (1 - a_b_t)\n",
    "        cof3 = torch.sqrt(b_t)\n",
    "\n",
    "        x_t = cof1 * x_0_hat + cof2 * x_t + cof3 * z\n",
    "\n",
    "    return x_t\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Do Not Modify\n",
    "class DDPM(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            unet: ConditionalDenoisingUNet,\n",
    "            betas: tuple[float, float] = (1e-4, 0.02),\n",
    "            num_ts: int = 300,\n",
    "            p_uncond: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.unet = unet\n",
    "        self.betas = betas\n",
    "        self.num_ts = num_ts\n",
    "        self.p_uncond = p_uncond\n",
    "        self.ddpm_schedule = ddpm_schedule(betas[0], betas[1], num_ts)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, c: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (N, C, H, W) input tensor.\n",
    "            c: (N,) int64 condition tensor.\n",
    "\n",
    "        Returns:\n",
    "            (,) diffusion loss.\n",
    "        \"\"\"\n",
    "        return ddpm_forward(\n",
    "            self.unet, self.ddpm_schedule, x, c, self.p_uncond, self.num_ts\n",
    "        )\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def sample(\n",
    "            self,\n",
    "            c: torch.Tensor,\n",
    "            img_wh: tuple[int, int],\n",
    "            guidance_scale: float = 5.0\n",
    "    ):\n",
    "        return ddpm_cfg_sample(\n",
    "            self.unet, self.ddpm_schedule, c, img_wh, self.num_ts, guidance_scale\n",
    "        )"
   ],
   "metadata": {
    "id": "gdQFWwIt8mXh"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### 3.3 Train your class-conditioned denoiser",
   "metadata": {
    "id": "EEGqlFNClOaw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Hyper parameters - Modify if you wish\n",
    "num_hidden = 128\n",
    "batch_size = 64\n",
    "num_epochs = 20\n",
    "lr = 1e-3\n",
    "img_wh = (28, 28)\n",
    "eval_batch_size = 20\n",
    "T = 300\n",
    "\n",
    "train_data = MNIST(root='./data', train=True, download=True, transform=transforms.Compose([\n",
    "    ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "]))\n",
    "test_data = MNIST(root='./data', train=False, download=True, transform=transforms.Compose([\n",
    "    ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "]))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "eval_loader = DataLoader(test_data, batch_size=eval_batch_size, shuffle=True)\n",
    "\n",
    "denosier_unet = ConditionalDenoisingUNet(\n",
    "    in_channels=1,\n",
    "    num_classes=10,\n",
    "    num_hiddens=num_hidden\n",
    ")\n",
    "ddpm = DDPM(denosier_unet, num_ts=T, p_uncond=0.1)\n",
    "\n",
    "optimizer = torch.optim.Adam(ddpm.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "    optimizer=optimizer, gamma=0.1 ** (1.0 / num_epochs)\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "ddpm.to(device)\n",
    "\n",
    "train_batch_losses = []\n",
    "train_epoch_losses = []\n",
    "\n",
    "run_id = f\"cond{randint(0, 9999):04d}\"\n",
    "base_dir = os.path.join('runs', run_id)\n",
    "plot_dir = os.path.join(base_dir, 'plots')\n",
    "weight_dir = os.path.join(base_dir, 'weights')\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "os.makedirs(weight_dir, exist_ok=True)\n",
    "\n",
    "tqdm.write(f\"start training {run_id} on device: {device}\")\n",
    "\n",
    "total_iters = num_epochs * len(train_loader)\n",
    "\n",
    "try:\n",
    "    pbar = tqdm(total=total_iters, desc='Training', dynamic_ncols=True)\n",
    "    for epoch in range(num_epochs):\n",
    "        ddpm.train()  # Set the model to training mode\n",
    "        epoch_loss = 0.0\n",
    "        for batch, (data, labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            loss = ddpm(data, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_loss = loss.item()\n",
    "            train_batch_losses.append(batch_loss)\n",
    "            epoch_loss += batch_loss\n",
    "\n",
    "            pbar.set_description(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "            pbar.set_postfix(loss=f\"{batch_loss:.4f}\")\n",
    "            pbar.update(1)\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "        train_epoch_losses.append(avg_epoch_loss)\n",
    "        scheduler.step()\n",
    "\n",
    "        ddpm.eval()\n",
    "        with torch.no_grad():\n",
    "            # sample eval_batch_size random class labels\n",
    "            sample_labels = torch.randint(0, 10, (eval_batch_size,), device=device)\n",
    "            samples = ddpm.sample(sample_labels, img_wh, guidance_scale=5.0).cpu()\n",
    "\n",
    "        # denormalize to [0,1]\n",
    "        samples = (samples + 1) / 2.0\n",
    "        n_cols = int(np.ceil(eval_batch_size ** 0.5))\n",
    "        n_rows = int(np.ceil(eval_batch_size / n_cols))\n",
    "\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 2, n_rows * 2))\n",
    "        axes = axes.flatten()\n",
    "        for idx, (img, lbl) in enumerate(zip(samples, sample_labels.cpu())):\n",
    "            axes[idx].imshow(img.squeeze(0), cmap='gray')\n",
    "            axes[idx].axis('off')\n",
    "            axes[idx].set_title(str(lbl.item()), fontsize=8, pad=2)  # label under each image\n",
    "\n",
    "        # turn off any empty subplots\n",
    "        for ax in axes[len(samples):]:\n",
    "            ax.axis('off')\n",
    "\n",
    "        plt.suptitle(f'Samples at Epoch {epoch + 1}', y=0.92)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(plot_dir, f'samples_epoch_{epoch + 1:02d}.png'),\n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        torch.save(ddpm.state_dict(), os.path.join(\n",
    "            weight_dir, f'ddpm_epoch_{epoch + 1:02d}.pt'))\n",
    "except KeyboardInterrupt:\n",
    "    tqdm.write(\"Training interrupted by user.\")\n",
    "finally:\n",
    "    pbar.close()\n",
    "\n",
    "    # ---------- Loss plots ----------\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(train_batch_losses, alpha=0.8, label='Batch loss')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('MSE loss')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plot_dir, 'batch_loss.png'), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(train_epoch_losses, linewidth=2, label='Epoch loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE loss')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plot_dir, 'epoch_loss.png'), dpi=300)\n",
    "    plt.close()"
   ],
   "metadata": {
    "id": "MkAIikcEMFEm"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### 3.4 Experiment with different guidance sacles",
   "metadata": {
    "id": "m4iTw-TFGYhA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "@torch.inference_mode()\n",
    "def experiment_guidance_scales(\n",
    "    model_dir: str,\n",
    "    img_wh: tuple[int, int] = (28, 28),\n",
    "    num_ts: int = 300,\n",
    "    num_classes: int = 10,\n",
    "    num_samples_per_class: int = 1,\n",
    "    guidance_scales: list[float] = [0, 1, 5, 7, 10, 15],\n",
    "    output_dir: str = \"guidance_experiment_outputs\"\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load model\n",
    "    unet = ConditionalDenoisingUNet(in_channels=1, num_classes=num_classes, num_hiddens=128)\n",
    "    ddpm_model = DDPM(unet, num_ts=num_ts)\n",
    "    latest_ckpt = sorted(os.listdir(os.path.join(model_dir, \"weights\")))[-1]\n",
    "    ddpm_model.load_state_dict(torch.load(os.path.join(model_dir, \"weights\", latest_ckpt)))\n",
    "    ddpm_model = ddpm_model.to(device)\n",
    "    ddpm_model.eval()\n",
    "\n",
    "    all_samples = {}\n",
    "\n",
    "    for g_scale in tqdm(guidance_scales, desc=\"Guidance scales\"):\n",
    "        images = []\n",
    "        labels = []\n",
    "\n",
    "        for digit in range(num_classes):\n",
    "            class_batch = torch.full((num_samples_per_class,), digit, dtype=torch.long, device=device)\n",
    "            sample = ddpm_model.sample(c=class_batch, img_wh=img_wh, guidance_scale=g_scale)\n",
    "            sample = torch.clamp((sample + 1) / 2, 0.0, 1.0)\n",
    "            images.append(sample)\n",
    "            labels += [digit] * num_samples_per_class\n",
    "\n",
    "        images = torch.cat(images, dim=0)\n",
    "        grid = torchvision.utils.make_grid(images, nrow=num_classes, padding=2)\n",
    "\n",
    "        plt.figure(figsize=(num_classes * 1.5, 2))\n",
    "        plt.imshow(grid.permute(1, 2, 0).cpu())\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Guidance scale = {g_scale}\")\n",
    "        plt.savefig(os.path.join(output_dir, f\"guidance_{g_scale:.1f}.png\"), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        all_samples[g_scale] = (images.cpu(), labels)\n",
    "\n",
    "    return all_samples\n",
    "\n",
    "\n",
    "samples = experiment_guidance_scales(\n",
    "        model_dir=\"runs/cond8049\",\n",
    "        output_dir=\"guidance_grids\"\n",
    "    )\n"
   ],
   "metadata": {
    "id": "d9gnGqPOoXT2"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "ryMrLOORbWLz",
    "k1iFNqV8HjzM",
    "GCawfhu0HqcH",
    "t9JhNQN5Ad3V",
    "0nyxOM-RbZnC",
    "m4iTw-TFGYhA"
   ],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
